{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split(X: np.ndarray, Y: np.ndarray, test_size:float=0.3, shuffle:bool=True):\n",
    "    indices = np.arange(X.shape[0])\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_end_ind = int(X.shape[0] * (1-test_size)+1)\n",
    "\n",
    "    return X[:train_end_ind], Y[:train_end_ind], X[train_end_ind:], Y[train_end_ind:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BatchGenerator:  # TESTED\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, batch_size: int, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        X: np.ndarray\n",
    "            Входные данные, размеры [n_samples, n_features]\n",
    "        Y: np.ndarray\n",
    "            Метки данных, размеры [n_samples] или [n_samples, n_classes]\n",
    "        batch_size: int\n",
    "            Размер мини-батча\n",
    "        shuffle: bool\n",
    "            Перемешивать ли данные перед каждой эпохой\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = X.shape[0]  # Количество примеров\n",
    "        self.indices = np.arange(self.num_samples)  # Индексы для перемешивания\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Итератор для перебора батчей.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            # print(\"Перемешано\")\n",
    "            # Перемешиваем индексы в начале каждой эпохи\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        \n",
    "        # print(self.indices)\n",
    "        # Возвращаем батчи данных\n",
    "        for start_idx in range(0, self.num_samples - self.batch_size + 1, self.batch_size):\n",
    "            end_idx = start_idx + self.batch_size  \n",
    "            batch_indices = self.indices[start_idx:end_idx]\n",
    "            # print(batch_indices)\n",
    "            yield self.X[batch_indices], self.Y[batch_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Возвращает количество батчей за одну эпоху.\n",
    "        \"\"\"\n",
    "        return (self.num_samples // self.batch_size)\n",
    "\n",
    "\n",
    "\n",
    "class ModelLayer:\n",
    "    def __init__(self):\n",
    "        self.previous_layer = None\n",
    "        self.next_layer = None\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "        self.backprop_gradient = None\n",
    "        self.grad_kostil = 1\n",
    "        self.training = True\n",
    "        self.lr = None\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None \n",
    "        self.Weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "        pass\n",
    "    \n",
    "    def backprop(self, grad: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "        pass\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "    def apply_gradient(self):\n",
    "        pass        \n",
    "\n",
    "\n",
    "class Softmax(ModelLayer): # TESTED\n",
    "    \"\"\"Класс реализующий софтмакс\n",
    "        Примеры использования:\n",
    "\n",
    "        num_class = 3\n",
    "        X = (np.random.random_sample((10, num_class)) * 10)\n",
    "        print(X)\n",
    "        Y = np.argmax(X, axis=1)\n",
    "        print(Y)\n",
    "\n",
    "        softmax = Softmax()\n",
    "        my_softmax = softmax(X)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "        #Решаем проблему огромных экспонент\n",
    "        # print(f\"{X=}\")\n",
    "        exp_pred = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        # exp_pred = np.where(exp_pred==0, 1e-10, exp_pred)\n",
    "        # print(\"EXPPRED:\", exp_pred)\n",
    "        # print('norm exp', np.exp(X))\n",
    "        self.output = exp_pred / np.sum(exp_pred, axis=1, keepdims=True) \n",
    "        # print(f\"{self.output=}\")\n",
    "        return self.output\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Текущая self.y_proba {self.output}\"\n",
    "    \n",
    "    def add_regulaizer(self, regulaizer:int):\n",
    "        self.grad_kostil = 1/regulaizer\n",
    "        print(self.grad_kostil)\n",
    "    \n",
    "    # def backprop(self, Y_real: np.ndarray):\n",
    "    #     Y_real = one_hot(Y_real)\n",
    "    #     self.backprop_gradient = (self.output - Y_real) * self.grad_kostil\n",
    "    #     return self.backprop_gradient\n",
    "\n",
    "    def backprop(self, grad: np.ndarray, debug:bool = False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "\n",
    "        self.backprop_gradient = grad\n",
    "        return self.backprop_gradient\n",
    "\n",
    "\n",
    "class CrossEntropyLoss: # TESTED\n",
    "    \"\"\"Класс реализующий кроссэнтропию\n",
    "        Y_real ПОДАЁТСЯ В ВИДЕ ОДНОМЕРНОГО ВЕКТОРА!!!!!!\n",
    "        Пример использования:\n",
    "        \n",
    "        softmax = Softmax()\n",
    "        my_loss = CrossEntropyLoss(softmax=softmax, reduction=\"sum\")\n",
    "        my_loss(logit_pred=X, Y_real=Y)\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, reduction = \"mean\"):\n",
    "        self.reduction = reduction\n",
    "        self.batch_size = 0\n",
    "        self.backprop_gradient = None\n",
    "\n",
    "    def __call__(self, softmax_pred: np.ndarray, Y_real: np.ndarray): # TESTED\n",
    "        # one hot encoding\n",
    "\n",
    "        self.batch_size = Y_real.shape[0]\n",
    "        Y_real = one_hot(Y_real, softmax_pred.shape[1])\n",
    "        # print(f\"{softmax_pred=}\")\n",
    "        # print(f\"{softmax_pred.shape}\")\n",
    "        # print(f\"{Y_real=}\")\n",
    "        # raise Exception(\"STOP\")\n",
    "        if self.reduction == \"sum\":\n",
    "            self.backprop_gradient = (softmax_pred - Y_real)\n",
    "            return -np.sum(Y_real * np.log(softmax_pred))\n",
    "        elif self.reduction == \"mean\":\n",
    "            #Надо вывести в оптимизатор\n",
    "            self.backprop_gradient = (softmax_pred - Y_real) / softmax_pred.shape[0]\n",
    "            return -np.sum(Y_real * np.log(softmax_pred))/softmax_pred.shape[0]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Метод не реализован\")\n",
    "    \n",
    "    \n",
    "    def get_backprop_gradient(self):\n",
    "        return self.backprop_gradient\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Linear(ModelLayer):\n",
    "    \"\"\"\n",
    "        Класс создающий линейный полносвязный слой\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_neuron:int, output_neuron:int):\n",
    "        super().__init__()\n",
    "        self.input_neuron = input_neuron\n",
    "        self.output_neuron = output_neuron\n",
    "        self.lr = None\n",
    "        self.init_weights()\n",
    "        \n",
    "    \n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "            print(f\"{X=}\")\n",
    "            print(f\"{self.Weights=}\")\n",
    "            \n",
    "        self.input = X.copy()\n",
    "        self.output = X @ self.Weights.T + self.bias \n",
    "        return self.output\n",
    "    \n",
    "\n",
    "    def backprop(self, grad: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "\n",
    "        self.backprop_gradient = grad @ self.Weights\n",
    "        self.weights_grad = grad.T @ self.input\n",
    "        self.bias_grad = np.sum(grad, axis=0)\n",
    "        return self.backprop_gradient\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        \n",
    "        weights_shape = (self.output_neuron, self.input_neuron)\n",
    "\n",
    "        self.bias = np.random.random_sample(self.output_neuron)+0.001\n",
    "        self.Weights = np.random.random_sample(weights_shape)-0.5\n",
    "        self.weights_grad = np.zeros(weights_shape)\n",
    "        self.bias_grad = np.zeros(self.output_neuron)\n",
    "\n",
    "    def apply_gradient(self):\n",
    "        self.Weights -= self.weights_grad\n",
    "        self.bias -= self.bias_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LeackyRelu(ModelLayer):\n",
    "    \"\"\"\n",
    "        Класс создающий слой активации LeakyRelu\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "\n",
    "        self.input = X.copy()\n",
    "        self.output = np.where(X >= 0, X, self.alpha * X)\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, grad: np.ndarray, debug:bool = False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "        self.backprop_gradient = np.where(self.input >=0, 1, self.alpha)\n",
    "        self.backprop_gradient = self.backprop_gradient * grad\n",
    "        return self.backprop_gradient\n",
    "\n",
    "\n",
    "\n",
    "class NeuroModel:\n",
    "    def __init__(self, *layers:ModelLayer, lr:np.float64 = 0.003):\n",
    "        self.layers = layers\n",
    "\n",
    "        self.layers[0].lr = lr\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].lr = lr\n",
    "            #Связывание слоёв для градиента\n",
    "            self.layers[i].previous_layer = self.layers[i - 1]\n",
    "            self.layers[i - 1].next_layer = self.layers[i]\n",
    "            \n",
    "\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, debug=debug)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, loss_grad:np.ndarray, debug:bool=False):\n",
    "        \n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backprop(grad, debug=debug)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Удобное представление для отображения слоев\"\"\"\n",
    "        return \" -> \".join([layer.__class__.__name__ for layer in self.layers])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, model:NeuroModel, beta1:float=0.9, beta2:float=0.99, debug=False): #learning_rate:float,\n",
    "        self.V_w = [np.zeros(layer.weights_grad.shape) if layer.weights_grad is not None else None  for layer in model.layers]\n",
    "        self.A_w = [np.zeros(layer.weights_grad.shape) if layer.weights_grad is not None else None  for layer in model.layers]\n",
    "        self.V_b = [np.zeros(layer.bias_grad.shape) if layer.bias_grad is not None else None for layer in model.layers]\n",
    "        self.A_b = [np.zeros(layer.bias_grad.shape) if layer.bias_grad is not None else None for layer in model.layers]\n",
    "        self.layer_amm = len(model.layers)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.model = model\n",
    "        # self.learning_rate = learning_rate\n",
    "        self.iteration = 1\n",
    "        self.debug = debug\n",
    "\n",
    "    def step(self):\n",
    "        for layer_i in range(self.layer_amm):\n",
    "            if self.debug:\n",
    "                print(self.V_w)\n",
    "\n",
    "            if self.V_w[layer_i] is None:\n",
    "                continue\n",
    "            \n",
    "            # Коррекция для весов\n",
    "            self.V_w[layer_i] = self.beta1 * self.V_w[layer_i] + (1 - self.beta1) * self.model.layers[layer_i].weights_grad\n",
    "            self.A_w[layer_i] = self.beta2 * self.A_w[layer_i] + (1 - self.beta2) * self.model.layers[layer_i].weights_grad ** 2\n",
    "\n",
    "            #Корректировка смещений\n",
    "            self.V_b[layer_i] = self.beta1 * self.V_b[layer_i] + (1 - self.beta1) * self.model.layers[layer_i].bias_grad\n",
    "            self.A_b[layer_i] = self.beta2 * self.A_b[layer_i] + (1 - self.beta2) * self.model.layers[layer_i].bias_grad ** 2\n",
    "\n",
    "            self.model.layers[layer_i].weights_grad = self.model.layers[layer_i].lr * ((self.V_w[layer_i] / (1 - self.beta1 ** self.iteration)) \n",
    "                                                                             / (np.sqrt(self.A_w[layer_i] / (1 - self.beta2 ** self.iteration)) + 0.0001))\n",
    "            self.model.layers[layer_i].bias_grad =  self.model.layers[layer_i].lr * ((self.V_b[layer_i]/(1-self.beta1 ** self.iteration)) \n",
    "                                                                           / (np.sqrt(self.A_b[layer_i] / (1 - self.beta2 ** self.iteration)) + 0.0001))\n",
    "\n",
    "            self.model.layers[layer_i].apply_gradient()\n",
    "\n",
    "        self.iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_im2col(X: np.ndarray, H_out:int, W_out:int, kernel_size:tuple, stride:tuple, dilation:tuple, in_chan:int=0, _reshape:bool=True) ->np.ndarray:\n",
    "    \"\"\"Im2col implementation using numpy memory access\n",
    "\n",
    "    :param X: Входной массив данных в формате shape[Количество батчей, количество каналов, высота, ширина]\n",
    "    :type X: np.ndarray\n",
    "    :param H_out: Высота выходного массива после операции свёртки. \n",
    "              H_out = 1 + (H_in + 2*padding[0] - dilation[0]  * (kernel_size[0] - 1) - 1) // stride[0]\n",
    "    :type H_out: int\n",
    "    :param W_out: Ширина выходного массива после операции свёртки.\n",
    "              W_out = 1 + (W_in + 2*padding[1] - dilation[1]  * (kernel_size[1] - 1) - 1) // stride[1]\n",
    "    :type W_out: int\n",
    "    :param kernel_size: Размер ядра свёртки\n",
    "    :type kernel_size: tuple\n",
    "    :param stride: Смещение по вертикали и горизонтали на 1 шаг. \n",
    "              Первый элемент - смещение по-вертикали, второй элемент - смещение по-горизонтали\n",
    "    :type stride: tuple\n",
    "    :param dilation: Расстояние между элементами ядра.\n",
    "              Первый элемент - расстояние по-вертикали, второй элемент - расстояние по-горизонтали\n",
    "    :type dilation: tuple\n",
    "    :return: Возвращает тензор являющийся im2col для входной матрицы X по ядру размером kernel_size\n",
    "    :rtype: np.ndarray\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    Пример использования приведён ниже:\n",
    "\n",
    "    X = np.arange(48).reshape(6, -1)\n",
    "    W = np.arange(6).reshape(2, 3)\n",
    "\n",
    "    print(X)\n",
    "    print(W)\n",
    "\n",
    "    H_in, W_in = X.shape\n",
    "    kernel_size = W.shape\n",
    "    padding = (0, 0)\n",
    "    dilation = (1, 1)\n",
    "\n",
    "    stride = (2, 1)\n",
    "\n",
    "    H_out = 1 + (H_in + 2*padding[0] - dilation[0]  * (kernel_size[0] - 1) - 1) // stride[0]\n",
    "    W_out = 1 + (W_in + 2*padding[1] - dilation[1]  * (kernel_size[1] - 1) - 1) // stride[1]\n",
    "\n",
    "    \n",
    "    my_im2col(X=X, H_out=H_out, W_out=W_out, kernel_size=kernel_size, stride=stride, dilation=dilation)\n",
    "\n",
    "    в np.lib.stride_tricks.as_strided \n",
    "    \n",
    "    За shape отвечают следующие величины:          shape=(num_batch, in_chan, H_out, W_out, *kernel_size)\n",
    "    Первое - Размер выходного батча\n",
    "    Второе - Количество выходных каналов\n",
    "    Третье - размер выходного тензора по высоте\n",
    "    Четвёртое - размер выходного тензора по длине\n",
    "    Пятое и Шестое - размер выходного тензора по высоте и длине ядра. \n",
    "        То есть то, какого размера должны быть индивидуальные матрицы которые будут переменожаться на ядра \n",
    "    \n",
    "    \n",
    "    За strides отвечают следующие элементы: для прохода по двумерной матрице двумеерной матрицей надо:\n",
    "\n",
    "    batch_stride, c_stride, h_stride, w_stride = X.strides\n",
    "    strides=(batch_stride, c_stride, h_stride*stride[0], w_stride*stride[1], h_stride*dilation[0], w_stride*dilation[1])\n",
    "\n",
    "    Первый элемент - Смещение по батчам\n",
    "\n",
    "    batch_stride\n",
    "\n",
    "    Второй элемент - Смещение по каналам\n",
    "\n",
    "    Третий элемент - Смещение по вертикали. \n",
    "        Так как мы играем с адресацией в памяти, то это ширина входной матрицы умноженная на количество строчек по которым происходит смещение,\n",
    "        В общем случае это  h_stride*stride[0]\n",
    "    \n",
    "        W_in*stride[0]  \n",
    "    \n",
    "    Четвёртый элемент - Смещение по горизонтали, w_stride*stride[1]\n",
    "\n",
    "    Пятый элемент - Крепкий орешек (нет). Разница между первым и последующим элементом, который будет передаваться в ядро по вертикали. \n",
    "        То есть по сути расстояние между элементами которые будут пихаться в ядро по вертикали.\n",
    "        Так как мы играем с адресацией в памяти, то это ширина входной матрица  умноженная на количество элементов по вертикали, которые мы хотим пропустить - 1. \n",
    "        Если этот параметр равен единицы, то строки берутся последовательно.\n",
    "        Иначе - пропускается n-1 строка.\n",
    "        В общем случае это h_stride*dilation[0]\n",
    "        W_in*dilation[0]\n",
    "    \n",
    "    Шестой элемент - то же что и третий, но по горизонтали. (Вспоминай гифку с матрицей как ситом)   w_stride*dilation[1]\n",
    "\n",
    "    Так как операции происходят с памятью надо умножить на 4. В теории, зависит от dtype, но тесты не подтвердили.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if X.ndim == 4:\n",
    "        num_batch = X.shape[0]\n",
    "        if not in_chan:\n",
    "            in_chan = X.shape[1]\n",
    "        batch_stride, c_stride, h_stride, w_stride = X.strides\n",
    "        # print(f\"{num_batch=}, {batch_stride, c_stride, h_stride, w_stride}, {in_chan=}, {H_out=}, {W_out=}\")\n",
    "        shapes = (num_batch, in_chan, H_out, W_out, *kernel_size)\n",
    "        strides = (batch_stride, c_stride, h_stride*stride[0], w_stride*stride[1], h_stride*dilation[0], w_stride*dilation[1])\n",
    "        \n",
    "                                                                                                        # (W_in*stride[0], stride[1], W_in*dilation[0], dilation[1])                 \n",
    "    elif X.ndim == 3:\n",
    "        if not in_chan:\n",
    "            in_chan = X.shape[0]\n",
    "        \n",
    "        c_stride, h_stride, w_stride = X.strides\n",
    "        # print(f\"X.ndim == 3\\n{c_stride, h_stride, w_stride}, {H_out=}, {W_out=}, {in_chan=}\")\n",
    "        shapes = (in_chan, H_out, W_out, *kernel_size)\n",
    "        strides = (c_stride, h_stride*stride[0], w_stride*stride[1], h_stride*dilation[0], w_stride*dilation[1])\n",
    "\n",
    "\n",
    "    elif X.ndim == 2:\n",
    "        h_stride, w_stride = X.strides\n",
    "        # print(f\"{h_stride, w_stride}, {H_out=}, {W_out=}\")\n",
    "        shapes = (H_out, W_out, *kernel_size)\n",
    "        strides = (h_stride*stride[0], w_stride*stride[1], h_stride*dilation[0], w_stride*dilation[1])\n",
    "\n",
    "        #return np.lib.stride_tricks.as_strided(X, shape=(num_batch, in_chan, H_out, W_out, *kernel_size), strides=(batch_stride, c_stride, h_stride*stride[0], w_stride*stride[1], h_stride*dilation[0], w_stride*dilation[1])).reshape(-1, kernel_size[0]*kernel_size[1])#*in_chan)\n",
    "        return np.lib.stride_tricks.as_strided(X, shape=shapes, strides=strides)\n",
    "\n",
    "    if _reshape:\n",
    "        reshape_shape = (num_batch, -1, kernel_size[0]*kernel_size[1])\n",
    "        return np.lib.stride_tricks.as_strided(X, shape=shapes, strides=strides).reshape(reshape_shape)#*in_chan)\n",
    "    \n",
    "    return np.lib.stride_tricks.as_strided(X, shape=shapes, strides=strides)\n",
    "\n",
    "def col2im(out:np.ndarray, num_batch:int, out_chan:int, H_out:int, W_out:int) -> np.ndarray:\n",
    "    \"\"\"Превращаем колонки в изображение.\n",
    "\n",
    "    :param out: получившееся изображение \n",
    "    :type out: np.ndarray\n",
    "    :param num_batch: Количество батчей\n",
    "    :type num_batch: int\n",
    "    :param out_chan: количество выходных каналов\n",
    "    :type out_chan: int\n",
    "    :param H_out: Высота выходного изображения\n",
    "    :type H_out: int\n",
    "    :param W_out: Ширина выходного изображения\n",
    "    :type W_out: int\n",
    "    :return: Новое изображение\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    return out.reshape(num_batch, out_chan, H_out, W_out)\n",
    "\n",
    "\n",
    "\n",
    "def add_padding(X:np.ndarray, padding:tuple):\n",
    "    \"\"\"\n",
    "    Добавляет паддинг к тензору формата [batch, channels, height, width]. \n",
    "    Или любому другому формату, главное, чтобы последними 2мя измерениями были высота и ширина\n",
    "\n",
    "    (0, 0) для осей batch и channels и тп, так как по ним паддинг не нужен.\n",
    "    (padding[0], padding[0]) и (padding[1], padding[1]) добавляют паддинг по высоте и ширине соответственно.\n",
    "\n",
    "    :param X: np.array, входной массив, к которому добавляется паддинг.\n",
    "    :param padding: tuple, (pad_height, pad_width), количество отступов по высоте и ширине.\n",
    "    :return: np.array, массив с добавленным паддингом.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(padding) != 2:\n",
    "        raise ValueError(\"Padding должен быть кортежем из двух элементов для высоты и ширины.\")\n",
    "    \n",
    "\n",
    "\n",
    "    paddings = [(0, 0)] * (X.ndim - 2) + [(padding[0], padding[0]), (padding[1], padding[1])]\n",
    "    return np.pad(\n",
    "                array=X, \n",
    "                pad_width = paddings, \n",
    "                mode='constant'\n",
    "            )\n",
    "\n",
    "def real_used_shape(H_out:int, W_out:int, kernel_size:tuple, stride:tuple, padding:tuple, dilation:tuple):\n",
    "    \"\"\"Считаем реально использующуюся входную размерность изображения для обратного распространения ошибки\"\"\"\n",
    "    H_in = (H_out - 1) * stride[0] - 2* padding[0] + dilation[0] * (kernel_size[0] - 1) + 1\n",
    "    W_in = (W_out - 1) * stride[1] - 2* padding[1] + dilation[1] * (kernel_size[1] - 1) + 1\n",
    "    return (H_in, W_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2d(ModelLayer):\n",
    "    def __init__(self, in_channels:int, kernels:int, kernel_size:tuple, stride:tuple=(1,1), padding:tuple=(0,0), dilation:tuple = (1,1)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        #self.out_channels = None\n",
    "        self.kernels = kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.H_out = None\n",
    "        self.W_out = None\n",
    "        self.H_in = None\n",
    "        self.W_in = None\n",
    "        self.num_batch=None\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        np.random.seed(3)\n",
    "\n",
    "        self.Weights = np.random.rand(self.kernels, self.kernel_size[0] * self.kernel_size[1]) - 0.5\n",
    "        # self.Weights = np.random.rand(self.out_channels, self.kernel_size[0] * self.kernel_size[1] * self.in_channels)\n",
    "        self.bias = np.random.rand(1, self.kernels) - 0.5\n",
    "\n",
    "    def rotate_kernel_180(self):\n",
    "        # rotate_kernel = self.Weights.reshape(-1, self.kernel_size[0]*self.kernel_size[1])\n",
    "        return self.Weights[:, ::-1].reshape(-1, *self.kernel_size)\n",
    "        # print(f\"{self.Weights=}\")\n",
    "        # print(f\"{rotate_kernel=}\")\n",
    "        # print(f\"{rotate_kernel[:, ::-1].reshape(-1, self.kernel_size[0]*self.kernel_size[1] * self.in_channels)=}\")\n",
    "        # return rotate_kernel[:, ::-1].reshape(-1, self.kernel_size[0]*self.kernel_size[1] * self.in_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "            print(f\"{X=}\")\n",
    "            print(f\"{self.Weights.T=}\")\n",
    "\n",
    "\n",
    "        self.num_batch = X.shape[0]\n",
    "        \n",
    "        self.H_in, self.W_in = X.shape[2], X.shape[3] # Обязательно должно быть до паддинга, иначе надо менять формулу H/W out убирая паддинг, а так же вычисление backprop\n",
    "\n",
    "        X = add_padding(X, self.padding)\n",
    "\n",
    "        # Сохраняем после паддинга, так как это влияет на размер выходных данных\n",
    "        # self.input = X.copy()\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        self.H_out = 1 + (self.H_in + 2*self.padding[0] - self.dilation[0]  * (self.kernel_size[0] - 1) - 1) // self.stride[0]\n",
    "        self.W_out = 1 + (self.W_in + 2*self.padding[1] - self.dilation[1]  * (self.kernel_size[1] - 1) - 1) // self.stride[1]\n",
    "\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{X=}\")\n",
    "            print(f\"{X.strides=}\")\n",
    "            print(f\"{self.H_in=}, {self.W_in=}\")\n",
    "            print(f\" {self.num_batch=} {self.kernel_size=} {self.W_out=},  {self.H_out=}\")\n",
    "            print(f\"{self.Weights.T.shape=}\")\n",
    "            print(f\"{self.bias=}\")\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        X = my_im2col(X=X, H_out=self.H_out, W_out=self.W_out, kernel_size=self.kernel_size, stride=self.stride, dilation=self.dilation)\n",
    "\n",
    "        self.input = X.copy()\n",
    "        \n",
    "\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"{X=}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        self.output = X @ self.Weights.T + self.bias\n",
    "        # self.output = X @ self.Weights.reshape(-1, self.out_channels)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"My new {self.output=}\")\n",
    "\n",
    "        # НЕ надо экранировать, так как интересующие нас вещи лежат на вертикали.\n",
    "        # Получаеися что сначала мы получаем все картинки полученные с помощью первого ядра, потом второго и тп.\n",
    "        self.output = col2im(np.transpose(self.output, axes=(0, 2, 1)), self.num_batch, self.kernels*self.in_channels, self.H_out, self.W_out)\n",
    "        \n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def backprop(self, grad: np.ndarray, debug:bool=False):\n",
    "        \n",
    "        kernel_backprop_grad = self.rotate_kernel_180()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"{self.stride[0]=}, {self.H_out=}, {self.stride[1]=}, {self.W_out=}\")\n",
    "\n",
    "        kernel_padding = (self.stride[0])  * (self.H_out - 1), (self.stride[1])  * (self.W_out - 1)\n",
    "\n",
    "        if debug:                                                            # stride в данном случае отвечает за смещение                    self.stride[0]  * (kernel_size[0] - 1)\n",
    "            print(f\"{kernel_backprop_grad} {(self.stride[0])=}, {(self.H_out - 1)=}, {(self.stride[1] + 1)=}, {(self.W_out - 1)=}, {(self.stride[0] * (self.H_out - 1)) + (self.H_out - 1)=}, {(self.stride[1])  * (self.W_out - 1)=}\")\n",
    "        \n",
    "        kernel_backprop_grad = add_padding(kernel_backprop_grad, kernel_padding)\n",
    "\n",
    "\n",
    "        \n",
    "        H_used, W_used = real_used_shape(H_out=self.H_out,\n",
    "                                         W_out=self.W_out,\n",
    "                                         kernel_size=self.kernel_size,\n",
    "                                         stride=self.stride,\n",
    "                                         padding=self.padding,\n",
    "                                         dilation=self.dilation)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"{self.H_in=}, {self.W_in=}, {self.H_out=}, {self.W_out=}, {H_used=}, {W_used=} \\n{kernel_backprop_grad.shape[0]-kernel_padding[0]=}, {kernel_backprop_grad.shape[1]-kernel_padding[1]=} {kernel_padding=}\")\n",
    "            print(f\"kernel_backprop_grad\\n{kernel_backprop_grad}\")\n",
    "        \n",
    "\n",
    "        #Добавляем in_chan так как мы должны получить ту же размерность что и в выходном тензоре, \n",
    "        # stride == (1, 1) так как по сути мы получаем при каждом смещении координаты той точки, откуда взяли свёртку\n",
    "        # TODO В BACKPROP есть большая проблема В СЛУЧАЕ self.dilation != (1,1)\n",
    "        self.backprop_gradient = my_im2col(kernel_backprop_grad, \n",
    "                                        #    H_out=kernel_backprop_grad.shape[0]-kernel_padding[0],\n",
    "                                        #    W_out=kernel_backprop_grad.shape[1]-kernel_padding[1], \n",
    "                                           H_out=H_used,\n",
    "                                           W_out=W_used,\n",
    "                                           kernel_size=(self.H_out, self.W_out), \n",
    "                                           stride=(1,1), \n",
    "                                           dilation=self.stride, \n",
    "                                         #  in_chan=self.kernels,\n",
    "                                           _reshape=False)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"self.backprop_gradient\\n{self.backprop_gradient}\")\n",
    "\n",
    "        # ВОТ ТУТ ПРОБЛЕМА В RESHAPE (Solver)\n",
    "        # self.backprop_gradient = self.backprop_gradient.reshape(-1, self.kernel_size[0]*self.kernel_size[1])\n",
    "        self.backprop_gradient = self.backprop_gradient.reshape(self.kernels, H_used*W_used, self.W_out*self.H_out)\n",
    "        if debug:\n",
    "            print(f\"self.backprop_gradient.reshape(self.kernels, -1, self.W_out*self.H_out)\\n{self.backprop_gradient=}\")\n",
    "\n",
    "        # print(f\"{kernel_backprop_grad.shape=}, {self.backprop_gradient.shape=} {self.H_in=} {self.W_in=}\")\n",
    "\n",
    "        \n",
    "            print(f\"GRAD\\n{grad.reshape(self.num_batch, self.kernels, self.in_channels, self.H_out * self.W_out)}\")\n",
    "        # print(f\"GRAD SUMMED \\n{grad.reshape(self.num_batch, self.kernels, self.in_channels, self.H_out * self.W_out).sum(axis=1)}\")\n",
    "            print(f\"GRAD.sum(axis=1).T\\n{np.transpose(grad.reshape(self.num_batch, self.kernels, self.in_channels, self.H_out * self.W_out), axes=(0, 1, 3,2))=}\")\n",
    "        \n",
    "        grad = grad.reshape(self.num_batch, self.kernels, self.in_channels, self.H_out * self.W_out)\n",
    "        grad_input = np.transpose(grad.reshape(self.num_batch, self.kernels, self.in_channels, self.H_out * self.W_out), axes=(0, 1, 3,2))\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"grad_input.T\\n{grad_input}\")\n",
    "\n",
    "        self.backprop_gradient =  self.backprop_gradient @ grad_input #np.einsum('...ij,...jk->...ik', self.backprop_gradient,  grad)\n",
    "\n",
    "\n",
    "        if debug:\n",
    "            print(f\"(kernel_backprop_grad @ grad.T)\\n{self.backprop_gradient}\")\n",
    "\n",
    "                                                                #было (2, -1)\n",
    "        self.backprop_gradient = np.sum(self.backprop_gradient, axis=( 1), keepdims=True)\n",
    "\n",
    "\n",
    "        # self.backprop_gradient = np.sum(self.backprop_gradient, axis=-4) #Это для отдельного расчёта по батчам\n",
    "\n",
    "        if debug:\n",
    "            print(f\"np.sum(self.backprop_gradient, axis=2), \\n{self.backprop_gradient}\")\n",
    "            # self.backprop_gradient = self.backprop_gradient.reshape(self.num_batch, self.in_channels, self.kernels, self.H_in, self.W_in) \n",
    "\n",
    "            # А вот тут не надо ничего транспонировать, так как из-за порядка операций мы уже получаем правильные данные  (ПРОВЕРИТЬ)\n",
    "\n",
    "            \n",
    "            print(f\"{H_used=}, {W_used=}\")\n",
    "\n",
    "        self.backprop_gradient = np.transpose(self.backprop_gradient, axes=(0,1,3,2))#.reshape(self.num_batch, \n",
    "                                                                # self.in_channels,\n",
    "                                                                # # self.kernels,\n",
    "                                                                # H_used,\n",
    "                                                                # W_used) \n",
    "        \n",
    "        if debug:\n",
    "            print(f'self.backprop_gradient RESHAPED\\n{self.backprop_gradient}')\n",
    "        \n",
    "        # self.backprop_gradient = np.sum(self.backprop_gradient, axis=2, keepdims=True)\n",
    "        # # self.backprop_gradient = np.sum(self.backprop_gradient, axis=-4) #Это для отдельного расчёта по батчам\n",
    "\n",
    "        if debug:\n",
    "            print(f\"np.sum(self.backprop_gradient, axis=2), \\n{self.backprop_gradient}\")\n",
    "        \n",
    "        self.backprop_gradient = np.pad(self.backprop_gradient, pad_width=((0,0), \n",
    "                                                                           (0,0), \n",
    "                                                                           (0, self.H_in - H_used), \n",
    "                                                                           (0, self.W_in - W_used)), mode=\"constant\")\n",
    "        \n",
    "        \n",
    "        if debug:\n",
    "            print(f\"self.backprop_gradient after reshape\\n{self.backprop_gradient}\")#self.kernels, in_chan=1, H_out=self.H_in, W_out=self.W_in)\n",
    "            # self.backprop_gradient = np.sum(self.backprop_gradient, axis=-3)\n",
    "            print(f\"{self.backprop_gradient=}\")\n",
    "        \n",
    "        # print(f\"LOOOOOK AT HIM!!!!!!\\n\\n\\n{grad1=}\")\n",
    "        # grad = grad.reshape(-1, self.kernels)\n",
    "\n",
    "        # print(f\"{grad=}\")\n",
    "\n",
    "        self.bias_grad = np.sum(grad, axis=(0,2, 3)) # np.zeros_like(self.bias)\n",
    "\n",
    "        # calc_w_grad = my_im2col(self.input, H_out=self.H_out, W_out=self.W_out, kernel_size=self.kernel_size, stride=self.stride, dilation=self.dilation)\n",
    "        \n",
    "\n",
    "\n",
    "        # print(f\"{calc_w_grad=}\")\n",
    "        \n",
    "        # self.weights_grad = grad.T @ calc_w_grad\n",
    "\n",
    "        # # Вычисление градиента весов по входным слоям учитывая паддинг, так как он влияет на выходные значения\n",
    "        if debug:\n",
    "            print(f\"grad\\n{grad}\")\n",
    "            # print(f\"grad.transpose(grad, axes=(0,1,3,2)) {np.transpose(grad, axes=(0,1,3,2))}\")\n",
    "            print(f\"grad.reshape(self.kernels, -1)\\n{grad.reshape(self.num_batch, self.kernels, self.H_out*self.W_out*self.in_channels)}\")\n",
    "            print(f\"self.input строки - это квадраты. Первая строка - первый квадрат и тп.\\n{self.input}\")\n",
    "            # print(f\"self.input.reshape(self.num_batch, self.in_channels, self.H_out*self.W_out, self.kernel_size[0]*self.kernel_size[1])\\n{self.input.reshape(self.num_batch, self.in_channels, self.H_out*self.W_out, self.kernel_size[0]*self.kernel_size[1])}\")\n",
    "        self.weights_grad = grad.reshape(self.num_batch, self.kernels, self.H_out*self.W_out*self.in_channels) @ self.input\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"self.weights_grad before sum\\n{self.weights_grad}\")\n",
    "        self.weights_grad = self.weights_grad.sum(axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "        if debug:\n",
    "            print(f\"{grad=}\")\n",
    "            print(f\"{self.backprop_gradient=}\")\n",
    "            print(f\"{self.weights_grad=}\")\n",
    "            print(f\"{self.Weights=}\")\n",
    "            print(f\"{self.bias_grad=}\")\n",
    "            # self.backprop_gradient222 = col2im(self.backprop_gradient[11111].T, self.num_batch, self.in_channels, self.H_in, self.W_in)\n",
    "        # print(f\"{self.backprop_gradient222=}\")\n",
    "        return self.backprop_gradient\n",
    "    \n",
    "    def apply_gradient(self):\n",
    "        self.Weights -= self.weights_grad\n",
    "        self.bias -= self.bias_grad\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool(ModelLayer):\n",
    "    def __init__(self, in_channels:int, kernel_size:tuple, stride:tuple=(1,1), padding:tuple=(0,0), dilation:tuple = (1,1)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        #self.out_channels = None\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.H_out = None\n",
    "        self.W_out = None\n",
    "        self.H_in = None\n",
    "        self.W_in = None\n",
    "        self.num_batch=None\n",
    "        self.active_zone = None\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug:bool=False):\n",
    "\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "            print(f\"{X=}\")\n",
    "\n",
    "\n",
    "        self.num_batch = X.shape[0]\n",
    "        \n",
    "        self.H_in, self.W_in = X.shape[2], X.shape[3] # Обязательно должно быть до паддинга, иначе надо менять формулу H/W out убирая паддинг, а так же вычисление backprop\n",
    "\n",
    "        X = add_padding(X, self.padding)\n",
    "        self.input = np.zeros_like(X)\n",
    "\n",
    "        self.H_out = 1 + (self.H_in + 2*self.padding[0] - self.dilation[0]  * (self.kernel_size[0] - 1) - 1) // self.stride[0]\n",
    "        self.W_out = 1 + (self.W_in + 2*self.padding[1] - self.dilation[1]  * (self.kernel_size[1] - 1) - 1) // self.stride[1]\n",
    "\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{X=}\")\n",
    "            print(f\"{X.strides=}\")\n",
    "            print(f\"{self.H_in=}, {self.W_in=}\")\n",
    "            print(f\" {self.num_batch=} {self.kernel_size=} {self.W_out=},  {self.H_out=}\")\n",
    "       \n",
    "        X = my_im2col(X=X, H_out=self.H_out, W_out=self.W_out, kernel_size=self.kernel_size, stride=self.stride, dilation=self.dilation)\n",
    "        if debug:\n",
    "            print(f\"X\\n{X}\")\n",
    "            print(np.max(X, axis=(-1), keepdims=True))\n",
    "        # print(np.argmax(X, axis=(-1,-2), keepdims=True))\n",
    "        \n",
    "        \n",
    "        # Можно и без np.transpose(self.output, axes=(0, 2, 1)) так как у нас количество входных каналов равно количеству выходных каналов\n",
    "        self.output = col2im(np.max(X, axis=-1, keepdims=True), num_batch=self.num_batch, out_chan=self.in_channels, H_out=self.H_out, W_out=self.W_out)\n",
    "        self.active_zone = np.argmax(X, axis=-1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, grad: np.ndarray, debug:bool=False):\n",
    "        # print(f\"{self.input=}\")\n",
    "        # ВОТ РЕШЕНИЕ!!! ТОЛЬЕО НАДО ЗАМЕНИТЬ НА размерность ЯДЕР\n",
    "        # print(8//self.kernel_size[0], 8%self.kernel_size[1])\n",
    "        \n",
    "        stride_window = my_im2col(self.input, H_out=self.H_out, W_out=self.W_out, kernel_size=self.kernel_size, stride=self.stride, dilation=self.dilation, _reshape=False)\n",
    "        # print(stride_window)\n",
    "        \n",
    "        \n",
    "        # coded_index = np.array([[8], [8], [8], [8], [8], [8]])\n",
    "\n",
    "        # Раскодирование индексов для последних двух размерностей\n",
    "        real_index_row = (self.active_zone // self.kernel_size[1]).flatten()  # индексы по минус второй оси\n",
    "        real_index_col = (self.active_zone % self.kernel_size[0]).flatten()   # индексы по минус первой оси\n",
    "\n",
    "        # Расширяем индексы до нужных размеров\n",
    "        # Эти индексы соответствуют первым четырём осям destination\n",
    "\n",
    "        # batch_indices = np.array([0, 0, 0, 1, 1, 1])  # индексы по первым осям\n",
    "        batch_indices = np.repeat(np.arange(self.num_batch), self.H_out*self.W_out*self.in_channels)\n",
    "        chanel_index = np.tile(np.repeat(np.arange(self.in_channels), self.H_out*self.W_out), self.num_batch)\n",
    "        H_ishod_index = np.tile(np.repeat(np.arange(self.H_out), self.W_out), self.num_batch*self.in_channels)\n",
    "        W_ishod_index = np.tile(np.repeat(np.arange(self.W_out), self.H_out), self.num_batch*self.in_channels)\n",
    "        \n",
    "        # zone_indexes = np.array([0,1,2,0,1,2])\n",
    "        # zone_indexes = np.tile(np.arange(self.H_out*self.W_out) ,self.num_batch*self.in_channels)\n",
    "        if debug:\n",
    "            print(f\"{batch_indices=}\")\n",
    "            print(f\"{chanel_index=}\")\n",
    "            print(f\"{H_ishod_index=}\")\n",
    "            print(f\"{W_ishod_index=}\")\n",
    "            print(f\"{self.H_out=}\")\n",
    "            print(f\"{self.W_out=}\")\n",
    "            print(f\"{real_index_col=}\")\n",
    "            print(f\"{real_index_row=}\")\n",
    "            print(f\"{stride_window.shape=}\")\n",
    "\n",
    "        # Используем np.add.at для накопления значений\n",
    "        np.add.at(stride_window, (batch_indices, chanel_index,  H_ishod_index, W_ishod_index, real_index_row, real_index_col), grad.flatten())\n",
    "\n",
    "        if debug:\n",
    "            print(self.input)\n",
    "\n",
    "        \n",
    "        return self.input\n",
    "\n",
    "\n",
    "class Flatten(ModelLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shapes = None\n",
    "\n",
    "    def __call__(self, X: np.ndarray, debug=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "        \n",
    "        self.input_shapes = X.shape\n",
    "        self.output = X.reshape(self.input_shapes[0], -1)\n",
    "\n",
    "        if debug:\n",
    "            print(self.output)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, grad: np.ndarray, debug=False):\n",
    "        if debug:\n",
    "            print(self.__class__.__name__)\n",
    "\n",
    "        return grad.reshape(self.input_shapes)\n",
    "    \n",
    "\n",
    "    \n",
    "def one_hot(Y: np.ndarray, num_class) -> np.array: # TESDED\n",
    "    one_hot_Y = np.zeros((Y.shape[0], num_class)) \n",
    "    one_hot_Y[np.arange(Y.shape[0]), Y] += 1\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Загружаем набор данных MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# Разделяем данные на входы (X) и метки (y)\n",
    "X = mnist.data  # Входные данные (изображения)\n",
    "y = mnist.target  # Метки (цифры от 0 до 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49001\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "X = np.array(mnist.data, dtype=int)/255  # Входные данные (изображения)\n",
    "Y = np.array(mnist.target, dtype=int)  # Метки (цифры от 0 до 9)\n",
    "\n",
    "\n",
    "# print(y)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X=X, Y=Y) \n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_batch_gen = BatchGenerator(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = NeuroModel(\n",
    "    Conv2d(in_channels=1, kernels=4, kernel_size=(5,5), stride=(2,2), padding=(1,1)),\n",
    "    Conv2d(in_channels=4, kernels=8, kernel_size=(5,5), stride=(1,1)),\n",
    "    Conv2d(in_channels=32, kernels=2, kernel_size=(3,3), stride=(1,1)),\n",
    "    Conv2d(in_channels=64, kernels=2, kernel_size=(3,3), stride=(1,1)),\n",
    "    MaxPool(in_channels=128, kernel_size=(3,3), stride=(2,2)),\n",
    "    Flatten(),\n",
    "    Linear(input_neuron=512, output_neuron=256),\n",
    "    LeackyRelu(),\n",
    "    Linear(input_neuron=256, output_neuron=64),\n",
    "    LeackyRelu(),\n",
    "    Linear(input_neuron=64, output_neuron=10),\n",
    "    Softmax(),\n",
    "    lr=0.003\n",
    ")\n",
    "\n",
    "optimizer = AdamOptimizer(model=model)\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_class = 10\n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "start = time()\n",
    "for epoch in range(1):\n",
    "    for x,y in train_batch_gen:\n",
    "        x = x.reshape(batch_size, 1, 28, 28)\n",
    "        output = model(x, debug=False)\n",
    "        # print(output)\n",
    "        loss_zn = loss(output, y)\n",
    "        loss_arr.append(loss_zn)\n",
    "        \n",
    "        acc_arr.append(np.sum(np.argmax(output, axis=1)==y)/y.shape)\n",
    "        model.backward(loss_grad=loss.get_backprop_gradient())\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b88f80d970>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(loss_arr)), loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confusion_matrix(y_pred: np.ndarray, y_real: np.ndarray, num_classes:int, return_macro:bool = True):\n",
    "\n",
    "    conf_matr = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    # y_pred = np.argmax(y_pred_softmax, axis=1)==y_real\n",
    "    np.add.at(conf_matr, (y_real, y_pred), 1)\n",
    "\n",
    "    if return_macro:\n",
    "        \n",
    "        macro_tp = np.diag(conf_matr)\n",
    "        macro_matr = conf_matr - np.diagflat(macro_tp)\n",
    "        macro_fn = np.sum(macro_matr, axis=1)\n",
    "        macro_fp = np.sum(macro_matr, axis=0)\n",
    "\n",
    "        div = (macro_tp + macro_fp)\n",
    "        macro_precision = macro_tp / np.where(div>0, div, 1)\n",
    "        div = (macro_tp + macro_fn)\n",
    "        macro_recall = macro_tp / np.where(div>0, div, 1)\n",
    "\n",
    "\n",
    "        y_pred_onehot = one_hot(y_pred, num_classes)\n",
    "        y_real_onehot = one_hot(y_real, num_classes)\n",
    "\n",
    "        micro_tp =  np.sum(y_real_onehot * y_pred_onehot)\n",
    "        micro_fp = np.sum((1 - y_real_onehot) * y_pred_onehot)\n",
    "        micro_fn = np.sum(y_real_onehot * (1 - y_pred_onehot))\n",
    "\n",
    "        micro_precision = micro_tp / max(micro_tp + micro_fp, 1)\n",
    "        micro_recall = micro_tp / max(micro_tp + micro_fn, 1)\n",
    "\n",
    "        devided = micro_precision + micro_recall\n",
    "\n",
    "        if not devided:\n",
    "            devided = 1\n",
    "\n",
    "        micro_f1 = 2 * micro_precision * micro_recall / devided\n",
    "        macro_f1 = 2 * macro_precision * macro_recall / (macro_precision + macro_recall)\n",
    "\n",
    "\n",
    "        return conf_matr, micro_f1, np.mean(macro_f1), np.mean(macro_precision), np.mean(macro_recall)\n",
    "\n",
    "    return conf_matr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "pred_proba = model(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 9, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2061,    1,    1,    0,    1,    5,    0,    2,    3,    4],\n",
       "        [   5, 2259,    7,    1,   16,    1,    4,    3,   10,    1],\n",
       "        [  80,   15, 1812,   24,   31,   40,    7,   88,   16,   10],\n",
       "        [  38,    4,   22, 1802,    3,  177,    0,   40,   13,   56],\n",
       "        [   9,    1,    0,    0, 1984,    0,    2,    5,    2,   47],\n",
       "        [  52,    1,    3,   13,    3, 1782,    6,   16,    4,   27],\n",
       "        [  97,    5,    7,    0,   48,   57, 1797,    0,   10,    0],\n",
       "        [  24,   14,   14,    8,    4,    4,    0, 2074,    5,   70],\n",
       "        [  75,    6,   13,   16,   22,  145,    5,   26, 1601,  165],\n",
       "        [  25,    4,    0,    4,   62,   19,    0,   19,    7, 1927]],\n",
       "       dtype=int64),\n",
       " 0.9095195009286157,\n",
       " 0.9083375037888588,\n",
       " 0.914706973546991,\n",
       " 0.9091843266526156)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(pred_proba, axis=1), y_real=y_test, num_classes=10, return_macro=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
